I"¾<h1 id="å·ç§¯ç¥ç»ç½‘ç»œç»“æ„ä¼˜åŒ–ç»¼è¿°">å·ç§¯ç¥ç»ç½‘ç»œç»“æ„ä¼˜åŒ–ç»¼è¿°</h1>

<p><strong>å‚è€ƒæ–‡çŒ®æ ¼å¼</strong></p>

<p><a href="http://kns.cnki.net/KCMS/detail/11.2109.TP.20190710.1703.009.html">[1]æ—æ™¯æ ‹,å´æ¬£æ€¡,æŸ´æ¯…,å°¹å®é¹.å·ç§¯ç¥ç»ç½‘ç»œç»“æ„ä¼˜åŒ–ç»¼è¿°[J/OL].è‡ªåŠ¨åŒ–å­¦æŠ¥:1-14[2019-08-21].https://doi.org/10.16383/j.aas.c180275.</a></p>

<h4 id="ä¸“ä¸šå…³é”®è¯">ä¸“ä¸šå…³é”®è¯</h4>

<ul>
  <li>ç»“æ„ä¼˜åŒ–(structure optimization)</li>
  <li>ç½‘ç»œå‰ªæ(network pruning)</li>
  <li>å¼ é‡åˆ†è§£(tensor factorization)</li>
  <li>çŸ¥è¯†è¿ç§»(knowledge transferring)</li>
</ul>

<h2 id="1-ç½‘è·¯å‰ªæä¸ç¨€ç–åŒ–">1. ç½‘è·¯å‰ªæä¸ç¨€ç–åŒ–</h2>

<p><em>å‚è€ƒæ–‡çŒ®ï¼š</em> <a href="https://arxiv.org/abs/1607.03250">Network trimming: a
data-driven neuron pruning approach towards efficient deep
architectures</a>
å·ç§¯ç¥ç»ç½‘ç»œä»å·ç§¯å±‚åˆ°å…¨è¿æ¥å±‚å­˜åœ¨å¤§é‡çš„å†—ä½™å‚æ•°,å½“å‚æ•°è¶‹è¿‘äº0çš„æ—¶å€™å¯ä»¥è¿›è¡Œå‚æ•°è£å‰ªï¼Œå‰”é™¤æ‰ä¸é‡
è¦çš„è¿æ¥ã€èŠ‚ç‚¹ç”šè‡³å·ç§¯æ ¸ï¼Œä»¥è¾¾åˆ°ç²¾ç®€ç½‘ç»œç»“æ„çš„ç›®çš„.å¥½å¤„å¦‚ä¸‹ï¼š</p>

<ul>
  <li>æœ‰æ•ˆç¼“è§£äº†è¿‡æ‹Ÿåˆç°è±¡çš„å‘ç”Ÿ</li>
  <li>ç¨€ç–ç½‘ç»œåœ¨ä»¥ CSR (Compressed sparse row format, CSR) å’Œ CSC (Compressed sparse column format)ç­‰ç¨€ç–çŸ©é˜µå­˜å‚¨æ ¼å¼å­˜å‚¨äºè®¡ç®—æœºä¸­å¯å¤§å¹…é™ä½å†…å­˜å¼€é”€ã€‚</li>
  <li>è®­ç»ƒå‚æ•°çš„å‡å°‘ä½¿å¾—ç½‘ç»œè®­ç»ƒé˜¶æ®µå’Œé¢„æµ‹é˜¶æ®µèŠ±è´¹æ—¶é—´æ›´å°‘ã€‚</li>
</ul>

<p>ä¸»è¦æ–¹æ³•ï¼š</p>

<ul>
  <li>è®­ç»ƒä¸­ç¨€ç–çº¦æŸ
    <ul>
      <li>Collinsç­‰åœ¨å‚æ•°ç©ºé—´ä¸­é€šè¿‡è´ªå©ªæœç´¢å†³å®šéœ€è¦ç¨€ç–åŒ–çš„éšå«å±‚(<a href="https://arxiv.org/abs/1412.1442">Memory bounded deep convolutional networks</a>)</li>
      <li>è¿­ä»£ç¡¬é˜ˆå€¼ (Iter-ative hard thresholding, IHT)(<a href="https://arxiv.org/abs/1607.05423">Training skinny deep
neural networks with iterative hard thresholding methods</a>):
        <ul>
          <li>ç¬¬ä¸€æ­¥ä¸­å‰”é™¤éšå«èŠ‚ç‚¹é—´æƒå€¼è¾ƒå°çš„è¿æ¥ , ç„¶åå¾®è°ƒ (Fine-tune) å…¶ä»–é‡è¦çš„å·ç§¯æ ¸.</li>
          <li>ç¬¬äºŒæ­¥ä¸­æ¿€æ´»æ–­æ‰çš„è¿æ¥,é‡æ–°è®­ç»ƒæ•´ä¸ªç½‘ç»œä»¥è·å–æ›´æœ‰ç”¨çš„ç‰¹å¾</li>
        </ul>
      </li>
      <li>å‰å‘â€“åå‘åˆ‡åˆ†æ³•(Forward-backward splitting method)([19]):</li>
      <li>ç»“æ„åŒ–ç¨€ç–å­¦ä¹ (Structured sparsity learning, SSL)([20]):æ¥å­¦ä¹ åˆ°çš„ç¡¬ä»¶å‹å¥½å‹ç¨€ç–ç½‘ç»œä¸ä»…å…·æœ‰æ›´åŠ ç´§å‡‘çš„ç»“æ„ , è€Œä¸”è¿è¡Œé€Ÿåº¦å¯æå‡ 3 å€è‡³ 5 å€.</li>
      <li>ä»¥åˆ†ç»„å½¢å¼å‰ªæå·ç§¯æ ¸è¾“å…¥,ä»¥æ•°æ®é©±åŠ¨çš„æ–¹å¼è·å–æœ€ä¼˜æ„Ÿå—é‡ (Receptive field)([21])</li>
      <li>åˆ©ç”¨ä¸€ç³»åˆ—ä¼˜åŒ–æªæ–½å°†ä¸å¯å¾®åˆ†çš„ l 0 èŒƒæ•°æ­£åˆ™é¡¹åŠ å…¥åˆ°ç›®æ ‡å‡½æ•° ,å­¦ä¹ åˆ°çš„ç¨€ç–ç½‘ç»œä¸ä»…å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ , è€Œä¸”æå¤§åŠ é€Ÿäº†æ¨¡å‹è®­ç»ƒå’Œæ¨å¯¼è¿‡ç¨‹ã€‚([22])</li>
      <li>Dropout ä½œä¸ºä¸€ç§å¼ºæœ‰åŠ›çš„ç½‘ç»œä¼˜åŒ–æ–¹æ³• , å¯è¢«è§†ä¸ºç‰¹æ®Šçš„æ­£åˆ™åŒ–æ–¹æ³• , è¢«å¹¿æ³›ç”¨äºé˜²æ­¢ç½‘ç»œè®­ç»ƒè¿‡æ‹Ÿåˆ[23-24]</li>
      <li>è‡ªé€‚åº” Dropout:æ ¹æ®ç‰¹å¾å’Œç¥ç»å…ƒçš„åˆ†å¸ƒä½¿ç”¨ä¸åŒçš„å¤šé¡¹å¼é‡‡æ ·æ–¹å¼ , å…¶æ”¶æ•›é€Ÿåº¦ç›¸å¯¹äºæ ‡å‡†Dropout æé«˜ 50 %.[25]</li>
    </ul>
  </li>
  <li>è®­ç»ƒåå‰ªæ:ä»å·²æœ‰æ¨¡å‹å…¥æ‰‹ï¼Œæ¶ˆé™¤ç½‘ç»œä¸­çš„å†—ä½™ä¿¡æ¯ã€‚ç”±å‰ªæç²’åº¦ä¸åŒåˆ’åˆ†å¦‚ä¸‹[26]ï¼š
    <ul>
      <li>å±‚é—´å‰ªæ:å‡å°‘ç½‘ç»œæ·±åº¦</li>
      <li>ç‰¹å¾å›¾å‰ªæ:å‡å°‘ç½‘ç»œå®½åº¦</li>
      <li>kxkæ ¸å‰ªæ:å‡å°‘ç½‘ç»œå‚æ•°ï¼Œæå‡ç½‘ç»œæ€§èƒ½ï¼›</li>
      <li>
        <p>æ ¸å†…å‰ªæ:æå‡æ¨¡å‹æ€§èƒ½
<img src="https://wangpengcheng.github.io/img/2019-08-21-15-27-18.png" alt="å‰ªææ–¹å¼" /></p>
      </li>
      <li>æœ€ä¼˜è„‘æŸä¼¤(Optimal brain damage, OBD)[8]ï¼šé€šè¿‡ç§»é™¤ç½‘ç»œä¸­ä¸é‡è¦çš„è¿æ¥ ,åœ¨ç½‘ç»œå¤æ‚åº¦å’Œè®­ç»ƒè¯¯å·®ä¹‹é—´è¾¾åˆ°ä¸€ç§æœ€ä¼˜å¹³è¡¡çŠ¶æ€ , æå¤§åŠ å¿«äº†ç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹.</li>
      <li>æœ€ä¼˜è„‘æ‰‹æœ¯ (Optimal brain sur-geon, OBS)[9]:æŸå¤±å‡½æ•°ä¸­çš„ Hessian çŸ©é˜µæ²¡æœ‰çº¦æŸ,è¿™ä½¿å¾— OBS åœ¨å…¶ä»–ç½‘ç»œä¸­å…·æœ‰æ¯” OBD æ›´æ™®éçš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸Šè¿°ä¸¤ç§æ–¹æ³•éƒ½é¢ä¸´è€…ä¸¥é‡çš„ç½‘ç»œç²¾åº¦æŸå¤±ã€‚</li>
      <li>æ·±åº¦å‹ç¼© (Deep compression)[28]:ç»¼åˆåº”ç”¨äº†å‰ªæã€é‡åŒ–ã€ç¼–ç ç­‰æ–¹æ³• , åœ¨ä¸å½±å“ç²¾åº¦çš„å‰æä¸‹å¯å‹ç¼©ç½‘ç»œ 35 âˆ¼ 49 å€ , ä½¿å¾—æ·±åº¦å·ç§¯ç½‘ç»œç§»æ¤åˆ°ç§»åŠ¨è®¾å¤‡ä¸Šæˆä¸ºå¯èƒ½ã€‚</li>
      <li>é’ˆå¯¹å…¨è¿æ¥å±‚è¿›è¡Œå‰ªææ“ä½œï¼Œæ‘†è„±äº†å¯¹äºè®­ç»ƒæ•°æ®çš„ä¾èµ–[29]ã€‚</li>
      <li>åŠ¨æ€ç½‘ç»œæ‰‹æœ¯ (Dynamic network surgery)[30]:åœ¨å‰ªæè¿‡ç¨‹ä¸­æ·»åŠ äº†ä¿®å¤æ“ä½œï¼Œå¯ä»¥é‡æ–°æ¿€æ´»é‡è¦çš„æ“ä½œã€‚äº¤æ›¿è¿›è¡Œï¼Œæå¤§çš„æ”¹å˜äº†ç½‘ç»œå­¦ä¹ æ•ˆç‡</li>
      <li>ReLU æ¿€æ´»å‡½æ•°ç§»è‡³ WinogradåŸŸ , ç„¶åå¯¹ Winograd å˜æ¢ä¹‹åçš„æƒé‡è¿›è¡Œå‰ªæ[31]ã€‚</li>
      <li>LASSO æ­£åˆ™åŒ–å‰”é™¤å†—ä½™å·ç§¯æ ¸ä¸å…¶å¯¹åº”çš„ç‰¹å¾å›¾ , ç„¶åé‡æ„å‰©ä½™ç½‘ç»œ , å¯¹äºå¤šåˆ†æ”¯ç½‘ç»œä¹Ÿæœ‰å¾ˆå¥½çš„æ•ˆæœã€‚[32]</li>
      <li>å»é™¤å¯¹äºè¾“å‡ºç²¾åº¦å½±å“è¾ƒå°çš„å·ç§¯æ ¸ä»¥åŠå¯¹åº”çš„ç‰¹å¾å›¾[33]:</li>
      <li>ä¸€æ¬¡æ€§ (One-shot) ä¼˜åŒ–æ–¹æ³•:å¯è·å¾—60%âˆ¼70%çš„ç¨€ç–åº¦[26]:</li>
      <li>ThiNet[34]:åœ¨è®­ç»ƒå’Œé¢„æµ‹é˜¶æ®µåŒæ—¶å‹ç¼©å¹¶åŠ é€Ÿå·ç§¯ç¥ç»ç½‘ç»œ,ä»ä¸‹ä¸€å·ç§¯å±‚è€Œéå½“å‰å·ç§¯å±‚çš„æ¦‚ç‡ä¿¡æ¯è·å–å·ç§¯æ ¸çš„é‡è¦ç¨‹åº¦ , å¹¶å†³å®šæ˜¯å¦å‰ªæå½“å‰å·ç§¯æ ¸ , å¯¹äºç´§å‡‘å‹ç½‘ç»œä¹Ÿæœ‰ä¸é”™çš„å‹ç¼©æ•ˆæœã€‚
<img src="https://wangpengcheng.github.io/img/2019-08-21-15-44-48.png" alt="ç½‘ç»œå‰ªæå¯¹ä¸åŒç½‘ç»œçš„å‹ç¼©æ•ˆæœ" /></li>
    </ul>
  </li>
</ul>

<h2 id="2-å¼ é‡åˆ†è§£">2. å¼ é‡åˆ†è§£</h2>

<p>ç¥ç»ç½‘ç»œçš„ä¸»è¦è®¡ç®—é‡ï¼Œæ¥è‡ªäºå·ç§¯å±‚ã€‚ç½‘ç»œä»…ä»…éœ€è¦å¾ˆå°‘ä¸€éƒ¨åˆ†å‚æ•°å°±å¯ä»¥è¿›è¡Œå‡†ç¡®çš„é¢„æµ‹[35];å·ç§¯æ ¸æ˜¯å››ç»´å¼ é‡ã€‚å°†åŸå§‹å¼ é‡åˆ†è§£ä¸ºè‹¥å¹²ä½ç§©å¼ é‡ï¼Œå‡å°‘å·ç§¯æ“ä½œæ•°ç›®ã€‚</p>

<p><strong>å¸¸è§çš„å¼ é‡åˆ†è§£æ–¹æ³•æœ‰ï¼š</strong></p>

<ul>
  <li>CPåˆ†è§£ï¼š<img src="https://wangpengcheng.github.io/img/2019-08-21-15-56-02.png" alt="ä¸»è¦å…¬å¼" /></li>
  <li>Tuckeråˆ†è§£ï¼šå°†å·ç§¯åˆ†è§£ä¸ºä¸€ä¸ªæ ¸å¼ é‡ä¸è‹¥å¹²å› å­çŸ©é˜µã€‚æ˜¯ä¸€ç§é«˜é˜¶çš„ä¸»æˆåˆ†åˆ†ææ–¹æ³•ã€‚
<img src="https://wangpengcheng.github.io/img/2019-08-21-15-57-01.png" alt="ä¸»è¦å…¬å¼" /></li>
  <li>çŸ©é˜µå¥‡å¼‚å€¼åˆ†è§£ (Singular value decomposition, SVD)å¸¸ç”¨äºå…¨è¿æ¥å±‚åˆ†è§£:<img src="https://wangpengcheng.github.io/img/2019-08-21-15-58-19.png" alt="SVDå…¬å¼" /></li>
</ul>

<p><img src="https://wangpengcheng.github.io/img/2019-08-21-15-59-44.png" alt="å¼ é‡åˆ†è§£è¿‡ç¨‹" /></p>

<p>å›¾ 3 (a)ä¸­Wä¸ºåŸå§‹å¼ é‡æ•°æ®ç»´åº¦ä¸ºï¼š(d,d,i,o);å¤æ‚åº¦ä¸ºO(d^2<em>i</em>o);è¿›è¡Œbä¸­çš„å¼ é‡åˆ†è§£åå¤æ‚åº¦ä¸ºO(o<em>k)+O(d^2</em>k*i);å¤§å¤§é™ä½äº†å¤æ‚åº¦ï¼Œå¤æ‚åº¦é™ä½ä¸ºåŸæ¥çš„(o/k),kè¶Šå°å‹ç¼©æ•ˆæœè¶Šæ˜æ˜¾ã€‚</p>

<p>å¼ é‡çš„å…¸å‹å¼•ç”¨æ˜¯å°†é«˜ç»´ç¦»æ•£ä½™å¼¦å˜æ¢(Discrete cosine transform, DCT)åˆ†è§£ä¸ºä¸€ç³»åˆ—ä¸€ç»´DCTå˜æ¢ã€‚</p>

<p>å·ç§¯çš„å¼ é‡åˆ†è§£æ–¹æ³•ï¼š</p>

<ul>
  <li>åˆ†ç¦»å·ç§¯æ ¸å­¦ä¹ æ–¹æ³• (Learning separable filters)[36]:èƒ½å¤Ÿå°†åŸå§‹å·ç§¯æ ¸ç”¨ä½ç§©å·ç§¯æ ¸è¡¨ç¤º,å‡å°‘æ‰€éœ€å·ç§¯æ ¸æ•°é‡ä»¥é™ä½è®¡ç®—è´Ÿæ‹…ã€‚</li>
  <li>é€å±‚åˆ†è§£æ–¹æ³•[37]ï¼šæ¯å½“ä¸€ä¸ªå·ç§¯æ ¸è¢«åˆ†è§£ä¸ºè‹¥å¹²ä¸€é˜¶å¼ é‡åˆ™å›ºå®šæ­¤å·ç§¯æ ¸å¹¶åŸºäºä¸€ç§é‡æ„è¯¯å·®æ ‡å‡†ä»¥å¾®è°ƒå…¶ä½™å·ç§¯æ ¸ã€‚åœ¨åœºæ™¯æ–‡æœ¬è¯†åˆ«ä¸­å¯åŠ é€Ÿç½‘ç»œ4.5å€</li>
  <li>å…¨è¿æ¥å±‚å¥‡å¼‚å€¼åˆ†è§£[38]ï¼šå…¨è¿æ¥å±‚è¿›è¡Œå±•å¼€å¥‡å¼‚å€¼åˆ†è§£ï¼Œå¯ä»¥å¤§å¤§å‡å°‘ç½‘å¯ä»¥å‚æ•°ï¼Œå¹¶æå‡ç½‘ç»œé€Ÿåº¦ã€‚</li>
  <li>åŸºäºcpåˆ†è§£çš„å·ç§¯æ ¸å¼ é‡åˆ†è§£æ–¹æ³•[39]ï¼šç”¨æœ€å°äºŒä¹˜æ³•ï¼Œå°†å·ç§¯æ ¸è¿›è¡Œåˆ†è§£ä¸ºå››ä¸ªä¸€é˜¶å·ç§¯æ ¸å¼ é‡ã€‚å¹¶ä¸”è¡¨æ˜å¼ é‡åˆ†è§£å…·æœ‰æ­£åˆ™åŒ–æ•ˆæœã€‚</li>
  <li>çº¦æŸçš„å¼ é‡åˆ†è§£æ–°ç®—æ³•[40]ï¼šå°†éå‡¸ä¼˜åŒ–çš„å¼ é‡åˆ†è§£è½¬ï¼ŒåŒ–ä¸ºå‡¸ä¼˜åŒ–é—®é¢˜ , ä¸åŒç±»æ–¹æ³•ç›¸æ¯”æé€Ÿæ˜æ˜¾ã€‚</li>
  <li>éå¯¹ç§°å¼ é‡åˆ†è§£æ–¹æ³•[41]ï¼šåŠ é€Ÿæ•´ä½“ç½‘ç»œè¿è¡Œã€‚</li>
</ul>

<p><strong>ç½‘ç»œæ•´ä½“å‹ç¼©</strong></p>

<ul>
  <li>åŸºäºPCAç´¯ç§¯èƒ½é‡çš„ä½ç§©é€‰æ‹©æ–¹æ³•å’Œå…·æœ‰éå…ˆè¡Œçš„é‡æ„è¯¯å·®ä¼˜åŒ–æ–¹æ³•[41]:</li>
  <li>åŸºäºå˜åˆ†è´å¶æ–¯çš„ä½ç§©é€‰æ‹©æ–¹æ³•å’ŒåŸºäº Tucker å¼ é‡åˆ†è§£çš„æ•´ä½“å‹ç¼©æ–¹æ³•[42]ï¼šå°ºå¯¸å’Œè¿è¡Œæ—¶é—´éƒ½å¤§å¤§é™ä½ã€‚</li>
  <li>åˆ©ç”¨å¾ªç¯çŸ©é˜µå‰”é™¤ç‰¹å¾å›¾ä¸­çš„å†—ä½™ä¿¡æ¯,è·å–ç‰¹å¾å›¾ä¸­æœ€æœ¬è´¨çš„ç‰¹å¾[43],å‡å°‘å‚æ•°ä½†æ˜¯æ€§èƒ½ä¸å‡å°‘</li>
  <li>ç­‰æå‡ºäº†ä¸€ç§åŸºäºä¼˜åŒ–CPåˆ†è§£å…¨éƒ¨å·ç§¯å±‚çš„ç½‘ç»œå‹ç¼©æ–¹æ³•[44]ï¼šå…‹æœäº†ç”±äºPCåˆ†è§£å¸¦æ¥çš„ç½‘ç»œç²¾åº¦ä¸‹é™é—®é¢˜ã€‚</li>
</ul>

<p>å¼ é‡åˆ†è§£è¿‡åéƒ½éœ€è¦é‡æ–°è®­ç»ƒç½‘ç»œè‡³æ”¶æ•›ï¼Œè¿›ä¸€æ­¥åŠ å‰§äº†ç½‘ç»œè®­ç»ƒçš„å¤æ‚åº¦ã€‚</p>

<h2 id="3-çŸ¥è¯†è¿ç§»">3. çŸ¥è¯†è¿ç§»</h2>

<p>çŸ¥è¯†è¿ç§»æ˜¯å±äºè¿ç§»å­¦ä¹ çš„ä¸€ç§ç½‘ç»œç»“æ„ä¼˜åŒ–æ–¹æ³• , å³å°†æ•™å¸ˆç½‘ç»œ (Teacher networks) çš„ç›¸å…³é¢†åŸŸçŸ¥è¯†è¿ç§»åˆ°å­¦ç”Ÿç½‘ç»œ (Student networks)ä»¥æŒ‡å¯¼å­¦ç”Ÿç½‘ç»œçš„è®­ç»ƒã€‚</p>

<p>æ•™å¸ˆç½‘ç»œæ‹¥æœ‰è‰¯å¥½çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå­¦ç”Ÿç½‘ç»œæ‹¥æœ‰æ›´å¥½çš„é’ˆå¯¹æ€§å’Œæ€§èƒ½</p>

<p><img src="https://wangpengcheng.github.io/img/2019-08-21-16-32-50.png" alt="çŸ¥è¯†è¿ç§»è¿‡ç¨‹" /></p>

<p>æ³¨é‡Šè¿ç§»ä¸»è¦ç”±æ•™å¸ˆç½‘ç»œè·å–å’Œå­¦ç”Ÿç½‘ç»œè®­ç»ƒä¸¤éƒ¨åˆ†æ„æˆã€‚</p>

<ul>
  <li>æ•™å¸ˆç½‘ç»œï¼šä¸»è¦è¦æ±‚å‡†ç¡®ç‡</li>
  <li>
    <p>å­¦ç”Ÿç½‘ç»œ:å°‘æ•°æ•°æ®çš„å¿«é€Ÿè®­ç»ƒ</p>
  </li>
  <li>åŸºäºçŸ¥è¯†è¿ç§»çš„æ¨¡å‹å‹ç¼©æ–¹æ³•[45]ï¼š</li>
  <li>åˆ©ç”¨ logits ( é€šè¿‡ softmax å‡½æ•°å‰çš„è¾“å…¥å€¼ , å‡å€¼ä¸º 0) æ¥è¡¨ç¤ºå­¦ä¹ åˆ°çš„çŸ¥è¯†[46]:</li>
  <li>çŸ¥è¯†ç²¾é¦(Knowledge distilling, KD)[47]:é‡‡ç”¨åˆé€‚çš„ T å€¼ , å¯ä»¥äº§ç”Ÿä¸€ä¸ªç±»åˆ«æ¦‚ç‡åˆ†å¸ƒè¾ƒç¼“å’Œçš„è¾“å‡º(ç§°ä¸ºè½¯æ¦‚ç‡æ ‡ç­¾(Soft probability labels)).</li>
  <li>FitNet[48]:</li>
  <li>Net2Net[50]:åŸºäºå‡½æ•°ä¿ç•™å˜æ¢ (Function-preserving transfor-mation)å¯ä»¥å¿«é€Ÿåœ°å°†æ•™å¸ˆç½‘ç»œçš„æœ‰ç”¨ä¿¡æ¯è¿ç§»åˆ°æ›´æ·± ( æˆ–æ›´å®½ ) çš„å­¦ç”Ÿç½‘ç»œ</li>
  <li>äºæ³¨æ„åŠ›çš„çŸ¥è¯†è¿ç§»æ–¹æ³•[51]ï¼šä»ä½ã€ä¸­ã€é«˜ä¸‰ä¸ªå±‚æ¬¡è¿›è¡Œæ³¨æ„åŠ›è¿ç§»;æå¤§æ”¹å–„äº†æ®‹å·®ç½‘ç»œç­‰æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œçš„æ€§èƒ½</li>
  <li>ç»“åˆ Fisher å‰ªæä¸çŸ¥è¯†è¿ç§»çš„ä¼˜åŒ–æ–¹æ³•[52]: åˆ©ç”¨æ˜¾è‘—æ€§å›¾è®­ç»ƒç½‘ç»œå¹¶åˆ©ç”¨ Fisher å‰ªææ–¹æ³•å‰”é™¤å†—ä½™çš„ç‰¹å¾å›¾ , åœ¨å›¾åƒæ˜¾è‘—åº¦é¢„æµ‹ä¸­å¯åŠ é€Ÿç½‘ç»œè¿è¡Œå¤šè¾¾ 10 å€ã€‚</li>
  <li>çŸ¥è¯†è¿ç§»çš„ç«¯åˆ°ç«¯çš„å¤šç›®æ ‡æ£€æµ‹æ¡†æ¶[54]:è§£å†³äº†ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­å­˜åœ¨çš„æ¬ æ‹Ÿåˆé—®é¢˜ , åœ¨ç²¾åº¦ä¸é€Ÿåº¦æ–¹é¢éƒ½æœ‰è¾ƒå¤§æ”¹å–„ã€‚</li>
</ul>

<p>çŸ¥è¯†è¿ç§»æ–¹æ³•èƒ½å¤Ÿç›´æ¥åŠ é€Ÿç½‘ç»œè¿è¡Œè€Œä¸éœ€è¦è¾ƒé«˜ç¡¬ä»¶è¦æ±‚ï¼Œå¤§å¹…é™ä½äº†å­¦ç”Ÿç½‘ç»œå­¦ä¹ åˆ°ä¸é‡è¦ä¿¡æ¯çš„æ¯”ä¾‹ , æ˜¯ä¸€ç§æœ‰æ•ˆçš„ç½‘ç»œç»“æ„ä¼˜åŒ–æ–¹æ³• . ç„¶è€ŒçŸ¥è¯†è¿ç§»éœ€è¦ç ”ç©¶è€…ç¡®å®šå­¦ç”Ÿç½‘ç»œçš„å…·ä½“ç»“æ„ï¼Œå¯¹ç ”ç©¶è€…çš„æ°´å¹³æå‡ºäº†è¾ƒé«˜çš„è¦æ±‚ã€‚æ­¤å¤–,ç›®å‰çš„çŸ¥è¯†è¿ç§»æ–¹æ³•ä»…ä»…å°†ç½‘ç»œè¾“å‡ºæ¦‚ç‡å€¼ä½œä¸ºä¸€ç§é¢†åŸŸçŸ¥è¯†è¿›è¡Œè¿ç§»ï¼Œæ²¡æœ‰è€ƒè™‘åˆ°æ•™å¸ˆç½‘ç»œç»“æ„å¯¹å­¦ç”Ÿç½‘ç»œç»“æ„çš„å½±å“ã€‚æå–æ•™å¸ˆç½‘ç»œçš„å†…éƒ¨ç»“æ„çŸ¥è¯† ( å¦‚ç¥ç»å…ƒ ) å¹¶æŒ‡å¯¼å­¦ç”Ÿç½‘ç»œçš„è®­ç»ƒï¼Œæœ‰å¯èƒ½ä½¿å­¦ç”Ÿç½‘ç»œè·å¾—æ›´é«˜çš„æ€§èƒ½ã€‚</p>

<h2 id="4-ç²¾ç»†æ¨¡å—è®¾è®¡">4. ç²¾ç»†æ¨¡å—è®¾è®¡</h2>

<p>é€šè¿‡å¯¹é«˜æ•ˆç²¾ç»†åŒ–æ¨¡å—çš„æ„é€ ï¼Œå¯ä»¥å®ç°ä¼˜åŒ–ç½‘ç»œç»“æ„çš„ç›®çš„ï¼Œé‡‡ç”¨æ¨¡å—åŒ–çš„ç½‘ç»œç»“æ„ä¼˜åŒ–æ–¹æ³•ï¼Œç½‘ç»œçš„è®¾è®¡ä¸æ„é€ æµç¨‹å¤§å¹…ç¼©çŸ­ã€‚ç›®å‰å°±æœ‰ä»£è¡¨æ€§çš„ç²¾ç»†æ¨¡å—æœ‰ï¼šInceptionæ¨¡å—ã€ç½‘ä¸­ç½‘ã€æ®‹å·®æ¨¡å—</p>

<h3 id="41-inceptionæ¨¡å—">4.1 Inceptionæ¨¡å—</h3>

<h4 id="411-inception-v1">4.1.1 Inception-v1</h4>
<p>Szegedy [4. ç­‰ä»ç½‘ä¸­ç½‘ (Net-work in network, NiN) [55. ä¸­å¾—åˆ°å¯å‘ï¼Œæå‡ºäº†å¦‚å›¾æ‰€ç¤ºçš„ Inception-v1 ç½‘ç»œç»“æ„ï¼š</p>

<p><img src="https://wangpengcheng.github.io/img/2019-08-21-18-51-11.png" alt="Inception ç»“æ„" /></p>

<p><strong>å°†ä¸åŒå°ºå¯¸çš„å·ç§¯æ ¸å¹¶è¡Œè¿æ¥èƒ½å¤Ÿå¢åŠ ç‰¹å¾æå–çš„å¤šæ ·æ€§</strong>ï¼›è€Œå¼•å…¥çš„ 1 Ã— 1 å·ç§¯æ ¸åˆ™åŠ é€Ÿäº†ç½‘ç»œè¿è¡Œè¿‡ç¨‹ã€‚</p>

<h4 id="412-inception-v2">4.1.2 Inception-v2</h4>

<p>å› ä¸ºå·ç§¯ç¥ç»ç½‘ç»œåœ¨è®­ç»ƒæ—¶ï¼Œæ¯å±‚ç½‘ç»œçš„è¾“å…¥åˆ†å¸ƒéƒ½ä¼šå‘ç”Ÿæ”¹å˜ï¼Œä¼šå¯¼è‡´æ¨¡å‹è®­ç»ƒé€Ÿåº¦é™ä½ã€‚å› æ­¤ä½¿ç”¨æ‰¹æ ‡å‡†åŒ–(Batch normalization BN)ã€‚ä¸»è¦ç”¨äºè§£å†³æ¿€æ´»å‡½æ•°ä¹‹å‰ï¼Œä½œç”¨æ˜¯è§£å†³æ¢¯åº¦é—®é¢˜[56]ã€‚</p>

<h4 id="413-inception-v357">4.1.3 Inception-v3[57]</h4>

<p>é™¤äº†å°† 7 Ã— 7 ã€ 5 Ã— 5 ç­‰è¾ƒå¤§çš„å·ç§¯æ ¸åˆ†è§£ä¸ºè‹¥å¹²è¿ç»­çš„ 3 Ã— 3 å·ç§¯æ ¸ , è¿˜å°† n Ã— n å·ç§¯æ ¸éå¯¹ç§°åˆ†è§£ä¸º 1 Ã— n å’Œ n Ã— 1 ä¸¤ä¸ªè¿ç»­å·ç§¯æ ¸ ( å½“ n = 7 æ—¶æ•ˆæœæœ€å¥½ ).</p>

<p><img src="https://wangpengcheng.github.io/img/2019-08-21-19-00-03.png" alt="å·ç§¯æ ¸åˆ†è§£" /></p>

<p>nception ç»“æ„ä¸æ®‹å·®ç»“æ„ç›¸ç»“åˆ , å‘ç°äº†æ®‹å·®ç»“æ„å¯ä»¥æå¤§åœ°åŠ å¿«ç½‘ç»œçš„è®­ç»ƒé€Ÿåº¦[58]:</p>

<p>Xception[59]:ç”¨å·ç§¯æ ¸å¯¹è¾“å…¥ç‰¹å¾å›¾è¿›è¡Œå·ç§¯æ“ä½œ.</p>

<p><img src="https://wangpengcheng.github.io/img/2019-08-21-21-12-12.png" alt="Xception æ¨¡å—" /></p>

<h3 id="42-ç½‘ä¸­ç½‘network-in-network">4.2 ç½‘ä¸­ç½‘ï¼ˆNetwork in networkï¼‰</h3>

<p>Mlpcover[55]ï¼šå³åœ¨å·ç§¯æ ¸åé¢æ·»åŠ ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºï¼Œ(Multilayer perceptron, MLP)ç”±äºå¤šå±‚æ„ŸçŸ¥æœºèƒ½å¤Ÿæ‹Ÿåˆä»»ä½•å‡½æ•° , å› æ­¤ Mlpconv ç»“æ„å¢å¼ºäº†ç½‘ç»œå¯¹å±€éƒ¨æ„ŸçŸ¥é‡çš„ç‰¹å¾è¾¨è¯†èƒ½åŠ›å’Œéçº¿æ€§è¡¨è¾¾èƒ½åŠ›ã€‚</p>

<p><img src="https://wangpengcheng.github.io/img/2019-08-22-20-42-32.png" alt="å¤šå±‚æ„ŸçŸ¥æœºå·ç§¯ç»“æ„" /></p>

<p>Maxout network in network(MIN)[56]:ç”¨Maxoutæ›¿ä»£ReLUè§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚ä¹‹å[57]ä½¿ç”¨ç¨€ç–è¿MLPå¹¶ä½¿ç”¨åˆ†ç¦»å·ç§¯(Unshared convolution)ç©ºé—´ç»´åº¦ä¸Šä½¿ç”¨å…±äº«å·ç§¯ã€‚å³å·ç§¯ä¸­çš„å·ç§¯ (Convolution in convolution, CiC)ã€‚</p>

<p>MPNIN(Mlpconv-wise supervised pretraining network in network)[62]ï¼šé€šè¿‡ç›‘ç£å¼é¢„å¤„ç†æ–¹æ³•åˆå§‹åŒ–ç½‘ç»œæ¨¡å‹çš„å„å±‚è®­ç»ƒå‚æ•° , å¹¶ç»“åˆæ‰¹æ ‡å‡†åŒ–ä¸ç½‘ä¸­ç½‘ç»“æ„èƒ½å¤Ÿè®­ç»ƒæ›´æ·±å±‚æ¬¡çš„å·ç§¯ç¥ç»ç½‘ç»œã€‚</p>

<p><strong>Mlpconv ç»“æ„å¼•å…¥äº†é¢å¤–çš„å¤šå±‚æ„ŸçŸ¥æœº,æœ‰å¯èƒ½ä¼šå¯¼è‡´ç½‘ç»œè¿è¡Œé€Ÿåº¦é™ä½,å¯¹æ­¤è¿›è¡Œæ”¹å–„å°†ä¼šæ˜¯æœªæ¥ç ”ç©¶çš„ä¸€ä¸ªæ–¹å‘.</strong></p>

<h3 id="43-æ®‹å·®æ¨¡å—">4.3 æ®‹å·®æ¨¡å—</h3>

<p>éšç€å·ç§¯ç¥ç»ç½‘ç»œé€æ¸å‘æ›´æ·±å±‚æ¬¡å‘å±• , ç½‘ç»œå°†é¢ä¸´é€€åŒ–é—®é¢˜è€Œä¸æ˜¯è¿‡æ‹Ÿåˆé—®é¢˜ , å…·ä½“è¡¨ç°åœ¨ç½‘ç»œæ€§èƒ½ä¸å†éšç€æ·±åº¦çš„å¢åŠ è€Œæå‡ , ç”šè‡³åœ¨ç½‘ç»œæ·±åº¦è¿›ä¸€æ­¥å¢åŠ çš„æƒ…å†µä¸‹æ€§èƒ½åè€Œå¿«é€Ÿä¸‹é™ã€‚</p>

<ul>
  <li>LSTM(Long short-term memory)[64]:ç”¨æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ—è·¯è¿æ¥çš„å¼•å…¥ï¼Œçªç ´äº†æ·±åº¦åœ¨è¾¾åˆ° 40 å±‚æ—¶ç½‘ç»œå°†é¢ä¸´é€€åŒ–é—®é¢˜çš„é™åˆ¶ï¼Œè¿›ä¸€æ­¥ä¿ƒè¿›äº†ç½‘ç»œæ·±åº¦çš„å¢åŠ [65]ã€‚</li>
  <li>æ®‹å·®ç½‘ç»œ(Residual network,ResNet):æ®‹å·®ç½‘ç»œçš„é—¨é™æœºåˆ¶ä¸å†æ˜¯å¯å­¦ä¹ çš„ , ä¹Ÿå³å§‹ç»ˆä¿æŒä¿¡æ¯ç•…é€šçŠ¶æ€ , è¿™æå¤§åœ°é™ä½äº†ç½‘ç»œå¤æ‚åº¦ , åŠ é€Ÿäº†ç½‘ç»œè®­ç»ƒè¿‡ç¨‹ , åŒæ—¶çªç ´äº†ç”±ç½‘ç»œé€€åŒ–å¼•èµ·çš„æ·±åº¦é™åˆ¶ã€‚
<img src="https://wangpengcheng.github.io/img/2019-08-22-22-04-25.png" alt="æ®‹å·®æ¨¡å—" />ã€‚</li>
</ul>

<p>ä¹‹åæ®‹å·®æ¨¡å—ä½¿å¾—ç½‘ç»œæ·±åº¦è¿›ä¸€æ­¥åŠ æ·±ï¼Œä½†æ˜¯åé¢å‘ç°ï¼Œæ·±åº¦å¹¶ä¸èƒ½å¾ˆå¥½çš„è¿›è¡Œå‚æ•°å’Œç‰¹å¾çš„å­¦ä¹ ï¼Œæœ‰äººè®¤ä¸ºç»ƒè¶…è¿‡50å±‚çš„ç½‘ç»œæ˜¯æ¯«æ— å¿…è¦çš„[69].å› æ­¤æ¥ä¸‹æ¥çš„ç½‘ç»œé€æ¸å‘å®½åº¦è¿›è¡Œé æ‹¢ã€‚é€šè¿‡å¢åŠ å®½åº¦å¯¹å…¶è¿›è¡Œæ›´æ”¹ã€‚</p>

<h3 id="44-å…¶å®ƒç²¾ç»†æ¨¡å—">4.4 å…¶å®ƒç²¾ç»†æ¨¡å—</h3>

<ul>
  <li>å…¨å‡å€¼æ± åŒ–(Global average pooling, GAP)[55]:ä»£æ›¿å…¨è¿æ¥å±‚ï¼Œç›¸å½“äºåœ¨æ•´ä¸ªç½‘ç»œç»“æ„ä¸Šåšæ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆã€‚</li>
  <li>å¯†é›†æ¨¡å— (Dense block):åœ¨ä»»ä½•ä¸¤å±‚ç½‘ç»œä¹‹é—´éƒ½æœ‰ç›´æ¥è¿æ¥[74]:æ”¹å–„äº†ç½‘ç»œä¸­ä¿¡æ¯ä¸æ¢¯åº¦çš„æµåŠ¨ï¼Œå¯¹äºç½‘ç»œå…·æœ‰æ­£åˆ™åŒ–çš„ä½œç”¨ã€‚</li>
  <li>è·¨è¿å·ç§¯ç¥ç»ç½‘ç»œ[75]ï¼šå…è®¸ç¬¬äºŒä¸ªæ± åŒ–å±‚è·¨è¿‡ä¸¤å±‚ç›´æ¥ä¸å…¨è¿æ¥å±‚ç›¸è¿æ¥ã€‚</li>
  <li>MobileNet å°†ä¼ ç»Ÿå·ç§¯è¿‡ç¨‹åˆ†è§£ä¸ºæ·±åº¦å¯åˆ†ç¦»å·ç§¯ (Depthwise convolution)å’Œé€ç‚¹å·ç§¯ (Pointwise convolution) ä¸¤æ­¥[77]</li>
  <li>åå‘æ®‹å·®æ¨¡å— (Inverted residual with linear bottleneck)[78]:ç­‰å°†æ®‹å·®æ¨¡å—ä¸æ·±åº¦å¯åˆ†ç¦»å·ç§¯ç›¸ç»“åˆã€‚</li>
  <li>ShuffleNet[79]ï¼šç­‰åœ¨MobileNet çš„åŸºç¡€ä¸Šè¿›ä¸€æ­¥æå‡ºäº†åŸºäºé€ç‚¹ç¾¤å·ç§¯(Pointwise group convolution) å’Œé€šé“æ··æ´— (Channel shuffle).</li>
</ul>

<h2 id="5-æ€»ç»“">5 æ€»ç»“</h2>

<p>ä¸»è¦ç ”ç©¶æ–¹å‘ï¼š</p>

<ul>
  <li>ç½‘ç»œå‰ªæä¸ç¨€ç–åŒ–():</li>
</ul>

<p>ç¨³å®šåœ°ä¼˜åŒ–å¹¶è°ƒæ•´ç½‘ç»œç»“æ„,ç›®å‰å¤§å¤šæ•°çš„æ–¹æ³•æ˜¯å‰”é™¤ç½‘ç»œä¸­å†—ä½™çš„è¿æ¥æˆ–ç¥ç»å…ƒ , è¿™ç§ä½å±‚çº§çš„å‰ªæå…·æœ‰éç»“æ„åŒ– (Non-structural) é£é™© ,åœ¨è®¡ç®—æœºè¿è¡Œè¿‡ç¨‹ä¸­çš„éæ­£åˆ™åŒ– (Irregular) å†…å­˜å­˜å–æ–¹å¼åè€Œä¼šé˜»ç¢ç½‘ç»œè¿›ä¸€æ­¥åŠ é€Ÿ.ä¸€äº›ç‰¹æ®Šçš„è½¯ç¡¬ä»¶æªæ–½èƒ½å¤Ÿç¼“è§£è¿™ä¸€é—®é¢˜,ç„¶è€Œä¼šç»™æ¨¡å‹çš„éƒ¨ç½²å¦ä¸€æ–¹é¢,å°½ç®¡ä¸€äº›é’ˆå¯¹å·ç§¯æ ¸å’Œå·ç§¯å›¾çš„ç»“æ„åŒ–å‰ªææ–¹æ³•èƒ½å¤Ÿè·å¾—ç¡¬ä»¶å‹å¥½å‹ç½‘ç»œ,åœ¨ CPUå’ŒGPUä¸Šé€Ÿåº¦æå‡æ˜æ˜¾,ä½†ç”±äºå‰ªæå·ç§¯æ ¸å’Œå·ç§¯é€šé“ä¼šä¸¥é‡å½±å“ä¸‹ä¸€éšå«å±‚çš„è¾“å…¥,æœ‰å¯
èƒ½å­˜åœ¨ç½‘ç»œç²¾åº¦æŸå¤±ä¸¥é‡çš„é—®é¢˜.</p>
<ul>
  <li>å¼ é‡åˆ†è§£():</li>
  <li>çŸ¥è¯†è¿ç§»</li>
  <li>ç²¾ç»†æ¨¡å—è®¾è®¡</li>
</ul>

<p>ä¸»è¦è¯„ä»·æŒ‡æ ‡ï¼š</p>

<ul>
  <li>å‡†ç¡®ç‡</li>
  <li>è¿è¡Œæ—¶é—´</li>
  <li>æ¨¡å‹å¤§å°</li>
  <li>æœ‰å¾…åŠ å…¥çš„æŒ‡æ ‡ï¼š
    <ul>
      <li>ä¹˜åŠ  (Multiply-and-accumulate )æ“ä½œé‡</li>
      <li>æ¨å¯¼æ—¶é—´</li>
      <li>æ•°æ®ååé‡</li>
      <li>ç¡¬ä»¶èƒ½è€—</li>
    </ul>
  </li>
</ul>

<p>è®¾è®¡ç¡¬ä»¶å‹å¥½å‹æ·±åº¦æ¨¡å‹å°†æœ‰åŠ©äºåŠ é€Ÿæ¨è¿›æ·±åº¦å­¦ä¹ çš„å·¥ç¨‹åŒ–å®ç°ï¼Œä¹Ÿæ˜¯ç½‘ç»œç»“æ„ä¼˜åŒ–çš„é‡ç‚¹ç ”ç©¶æ–¹å‘ã€‚</p>

<h3 id="å‚è€ƒæ–‡çŒ®åˆ—è¡¨">å‚è€ƒæ–‡çŒ®åˆ—è¡¨</h3>

<ol>
  <li>Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks. In: Proceedings of Neural Information Processing Systems. Lake Tahoe,Nevada, USA: Curran Associates Inc, 2012. 1097âˆ’1105</li>
  <li>Zeiler M D, Fergus R. Visualizing and understanding convolutional networks. In: Proceedings of European Conference on Computer Vision. Zurich, Switzerland: Springer, 2014.818âˆ’833</li>
  <li><a href="https://arxiv.org/abs/1409.1556">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition.arXiv: 1409.1556,2014.</a></li>
  <li>Szegedy C, Liu W, Jia Y Q, Sermanet P, Reed S, Anguelov D. Going deeper with convolutions. In: Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. IEEE: Boston, USA: 2015. 1âˆ’9</li>
  <li>He K M, Zhang X Y, Ren S Q, Sun J. Deep residual learning for image recognition. In: Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. IEEE: LasVegas, USA: 2016. 770âˆ’778</li>
  <li>LeCun Y, Bottou L, Bengio Y, Haffner P. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 1998, 86(11): 2278âˆ’2324</li>
  <li>He K M, Sun J. Convolutional neural networks at constrained time cost. In: Proceedings of Computer Vision and Pattern Recognition. Boston, USA: IEEE, 2015. 5353âˆ’5360</li>
  <li>LeCun Y, Denker J S, Solla S A. Optimal brain damage.In: Proceedings of Neural Information Processing Systems.MIT Press: Denver, Colorado, USA: 1990. 598âˆ’605</li>
  <li>Hassibi B, Stork D G, Wolff G. Optimal brain surgeon: extensions and performance comparisons. In: Proceedings of Neural Information Processing Systems. MIT Press: Denver, Colorado, USA: 1994. 263âˆ’270</li>
  <li><a href="https://arxiv.org/abs/1710.09282">Cheng Y, Wang D, Zhou P, Zhang T. A survey of model compression and acceleration for deep neural networks.arXiv: 1710.09282, 2017.</a></li>
  <li>Cheng J, Wang P, Li G, Hu Q H, Lu H Q. Recent advances in efficient computation of deep convolutional neural networks.Frontiers of Information Technology &amp; Electronic Engineering, 2018, 19(1): 64âˆ’77</li>
  <li>Lei Jie, Gao Xin, Song Jie, Wang Xing-Lu, Song Ming-Li.Compression of deep networks model: a review. Journal of Software, 2018, 29(02): 251âˆ’266( é›·æ° , é«˜é‘« , å®‹æ° , ç‹å…´è·¯ , å®‹æ˜é» . æ·±åº¦ç½‘ç»œæ¨¡å‹å‹ç¼©ç»¼è¿° . è½¯ä»¶å­¦æŠ¥ , 2018, 29(02):251-266.)</li>
  <li><a href="https://arxiv.org/abs/1607.03250">Hu H, Peng R, Tai Y W, Tang C K. Network trimming: a data-driven neuron pruning approach towards efficient deep architectures. arXiv: 1607.03250, 2016.</a></li>
  <li>Cheng Y, Wang D, Zhou P. Model Compression and Acceleration for Deep Neural Networks: The Principles, Progress,and Challenges. IEEE Signal Processing Magazine, 2018,35(1): 126âˆ’136</li>
  <li><a href="https://arxiv.org/abs/1412.6115">Gong Y, Liu L, Yang M, Bourdev L. Compressing deep convolutional networks using vector quantization. arXiv:1412.6115, 2014.</a></li>
  <li>Reed R. Pruning algorithms-a survey. IEEE Transactions on Neural Networks, 1993, 4(5): 740âˆ’747</li>
  <li><a href="https://arxiv.org/abs/1412.1442">Collins M D, Kohli P. Memory bounded deep convolutional networks. arXiv: 1412.1442, 2014.</a></li>
  <li><a href="https://arxiv.org/abs/1607.05423">Jin X J, Yuan X T, Feng J S, Yan S C. Training skinny deep neural networks with iterative hard thresholding methods. arXiv: 1607.05423, 2016.</a></li>
  <li>Zeiler M D, Fergus R. Less is more: towards compact CNNs. In: Proceedings of European Conference on Computer Vision. Amsterdam,The Netherlands: Springer, 2016.662âˆ’677</li>
  <li>Wen W, Wu C P, Wang Y D, Chen Y R, Li H. Learning structured sparsity in deep neural networks. In: Proceedings of Neural Information Processing Systems. Spain: 2016.2074âˆ’2082</li>
  <li>Lebedev V, Lempitsky V. Fast convnets using group-wise brain damage. In: Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. Las Vegas, USA:IEEE, 2016. 2554âˆ’2564</li>
  <li><a href="https://arxiv.org/abs/1712.01312">Louizos C, Welling M, Kingma D P. Learning sparse neural networks through L 0 regularization. arXiv: 1712.01312,2017.</a></li>
  <li>Hinton G E, Srivastava N, Krizhevsky A, Sutskever I,Salakhutdinov R R. Improving neural networks by preventing co-adaptation of feature detectors.arXiv: 1207.0580,2012.](https://arxiv.org/abs/1207.0580)</li>
  <li>Srivastava N, Hinton G, Krizhevsky A, Sutskever I,Salakhutdinov R.Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 2014, 15(1): 1929âˆ’1958</li>
  <li>Li Z, Gong B, Yang T. Improved dropout for shallow and deep learning. In: Proceedings of Neural Information Processing Systems. Spain: 2016.2523âˆ’2531</li>
  <li>Anwar S, Sung W. Coarse pruning of convolutional neural networks with random masks. In: International Conference on Learning Representation. France: 2017.134âˆ’145</li>
  <li>Hanson S J, Pratt L Y. Comparing biases for minimal network construction with back-propagation. In: Proceedings of Neural Information Processing Systems. Denver, Colorado, USA: 1989. 177âˆ’185</li>
  <li><a href="https://arxiv.org/abs/1510.00149">Han S, Mao H, Dally W J. Deep compression: compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv: 1510.00149,2015.</a></li>
  <li><a href="https://arxiv.org/abs/1507.06149">Srinivas S, Babu R V. Data-free parameter pruning for deep neural networks. arXiv: 1507.06149, 2015.</a></li>
  <li><a href="">Guo Y, Yao A, Chen Y. Dynamic network surgery for efficient DNNs. In: Proceedings of Neural Information Processing Systems. Denver, Colorado, USA: 2016. 1379âˆ’1387</a></li>
  <li><a href="">Liu X Y, Jeff Pool, Han S, William J.Dally. Efficient sparse-winograd convolutional neural networks. In: International Conference on Learning Representation. Canada: 2018.</a></li>
  <li><a href="">He Y, Zhang X, Sun J. Channel pruning for accelerating very deep neural networks. In: Proceedings of International Conference on Computer Vision. Venice, Italy: 2017. 6</a></li>
  <li><a href="https://arxiv.org/abs/1608.08710">Li H, Kadav A, Durdanovic I, Samet H, Graf H P. Pruning filters for efficient convNets. arXiv: 1608.08710, 2016.</a></li>
  <li><a href="">Denil M, Shakibi B, Dinh L, De F N. Predicting parameters in deep learning. In: Proceedings of Neural Information Processing Systems. Lake Tahoe, Nevada, United States: 2013. 2148âˆ’2156</a></li>
  <li><a href="">Denil M, Shakibi B, Dinh L, De F N. Predicting parameters in deep learning. In: Proceedings of Neural Information Processing Systems. Lake Tahoe, Nevada, United States: 2013.2148âˆ’2156</a></li>
  <li><a href="">Rigamonti R, Sironi A, Lepetit V, Fua P. Learning separable filters. In: Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Portland, OR, USA: IEEE,2013. 2754âˆ’2761</a></li>
  <li><a href="https://arxiv.org/abs/1405.3866">Jaderberg M, Vedaldi A, Zisserman A. Speeding up convolutional neural networks with low rank expansions. arXiv:1405.3866, 2014.</a></li>
  <li><a href="">Denton E, Zaremba W, Bruna J, LeCun Y, Fergus R. Exploiting linear structure within convolutional networks for efficient evaluation. In: Proceedings of Neural Information Processing Systems. Montreal, Quebec, Canada: 2014.1269âˆ’1277</a></li>
  <li><a href="https://arxiv.org/abs/1412.6553">Lebedev V, Ganin Y, Rakhuba M, Oseledets I, Lempitsky V.Speeding-up convolutional neural networks using fine-tuned cp-decomposition. arXiv: 1412.6553, 2014.</a></li>
  <li><a href="https://arxiv.org/abs/1511.06067">Tai C, Xiao T, Zhang Y, Wang X G. Convolutional neural networks with low-rank regularization. arXiv: 1511.06067, 2015.</a></li>
  <li><a href="">Zhang X, Zou J, Ming X, He K M, Sun J. Efficient and accurate approximations of nonlinear convolutional networkss.In: Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. Boston, USA: IEEE, 2015. 1984âˆ’1992</a></li>
  <li><a href="https://arxiv.org/abs/1511.06530">Kim Y D, Park E, Yoo S, Choi T, Yang L, Shin D. Compression of deep convolutional neural networks for fast and low power mobile applications. arXiv: 1511.06530, 2015.</a></li>
  <li><a href="">Wang Y, Xu C, Xu C, Tao D. Beyond filters: Compact feature map for portable deep model. In: Proceedings of International Conference on Machine Learning. Sydney, Australia: 2017. 3703âˆ’3711</a></li>
  <li><a href="">Astrid M, Lee S I. CP-decomposition with tensor power method for convolutional neural networks compression. In:Proceedings of IEEE Conference on Big Data and Smart Computing. Korea: IEEE, 2017. 115âˆ’118</a></li>
  <li><a href="">Bucila C, Caruana R, Niculescu-Mizil A. Model compression. In: Proceedings of The 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. Philadelphia, USA: 2006. 535âˆ’541</a></li>
  <li><a href="">Ba L J, Caruana R. Do deep nets really need to be deep?In: Proceedings of Neural Information Processing Systems.Montreal, Quebec, Canada: 2014. 2654âˆ’2662</a></li>
  <li><a href="https://arxiv.org/abs/1503.02531">Hinton G, Vinyals O, Dean J. Distilling the knowledge in a neural network. arXiv: 1503.02531, 2015.</a></li>
  <li><a href="https://arxiv.org/abs/1412.6550">(Romero A, Ballas N, Kahou S E, Chassang A, Gatta C, Bengio Y. Fitnets: hints for thin deep nets. arXiv: 1412.6550,2014)</a></li>
  <li><a href="">Luo P, Zhu Z, Liu Z, Wang X G, Tang X O. Face model compression by distilling knowledge from neurons. In: Proceedings of AAAI Conference on Artificial Intelligence. Phoenix,Arizona, USA: 2016. 3560âˆ’3566</a></li>
  <li><a href="https://arxiv.org/abs/1511.05641">Chen T, Goodfellow I, Shlens J. Net2net: accelerating learning via knowledge transfer. arXiv: 1511.05641, 2015.</a></li>
  <li><a href="">Zagoruyko S, Komodakis N. Paying more attention to attention: improving the performance of convolutional neural networks via attention transfer. In: International Conference on Learning Representation. France: 2017.</a></li>
  <li><a href="https://arxiv.org/abs/1801.05787">Lucas T, Iryna K, Alykhan T, Ferenc H. Faster gaze prediction with dense networks and Fisher pruning. arXiv:1801.05787, 2018.</a></li>
  <li><a href="">Yim J, Joo D, Bae J, Kim J. A gift from knowledge distillation: fast optimization, network minimization and transfer learning. In: Proceedings of Computer Vision and Pattern Recognition. Honolulu, HI, USA: IEEE, 2017.</a></li>
  <li><a href="">Chen G, Choi W, Yu X, Han T, Chandraker M. Learning efficient object detection models with knowledge distillation.In: Proceedings of Neural Information Processing Systems.US: 2017. 742âˆ’751</a></li>
  <li><a href="https://arxiv.org/abs/1312.4400">Lin M, Chen Q, Yan S. Network in network. arXiv:1312.4400, 2013.</a></li>
  <li><a href="https://arxiv.org/abs/1502.03167">Ioffe S, Szegedy C. Batch normalization: accelerating deep network training by reducing internal covariate shift. arXiv:1502.03167, 2015.</a></li>
  <li><a href="">Szegedy C, Vanhoucke V, Ioffe S, Shlens J, Wojna Z. Rethinking the inception architecture for computer vision. In:Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. Las Vegas, NV, USA: IEEE, 2016.2818âˆ’2826</a></li>
  <li><a href="">Szegedy C, Ioffe S, Vanhoucke V, Alemi A A. Inception-v4,inception-resnet and the impact of residual connections on learning. In: Proceedings of AAAI Conference on Artificial Intelligence. San Francisco, California USA: 2017. 12</a></li>
  <li><a href="https://arxiv.org/abs/1800-1807">Chollet F. Xception: Deep learning with depthwise separable convolutions. arXiv: 1800-1807, 2016.</a></li>
  <li><a href="https://arxiv.org/abs/1511.02583">Chang J R, Chen Y S. Batch-normalized maxout network in network. arXiv: 1511.02583, 2015.</a></li>
  <li><a href="">Han X, Dai Q. Batch-normalized mlpconv-wise supervised pre-training network in network. Applied Intelligence, 2018,48(1): 142âˆ’155</a></li>
  <li><a href="https://arxiv.org/abs/1505.00387">Srivastava R K, Greff K, Schmidhuber J. Highway networks.arXiv: 1505.00387, 2015.</a></li>
  <li><a href="">Hochreiter S, Schmidhuber J. Long short-term memory.Neural Computation, 1997, 9(8): 1735âˆ’1780</a></li>
  <li><a href="https://arxiv.org/abs/1605.07648">Larsson G, Maire M, Shakhnarovich G. Fractalnet: ultra-deep neural networks without residuals. arXiv:1605.07648,2016.</a></li>
  <li><a href="">Huang G, Sun Y, Liu Z, Sedra D, Weinberger K Q. Deep networks with stochastic depth. In: Proceedings of European Conference on Computer Vision. Amsterdam, The Nether-lands: Springer, 2016. 646âˆ’661</a></li>
  <li><a href="">He K M, Zhang X, Ren S, Sun J. Identity mappings in deep residual networks. In: Proceedings of European Conference on Computer Vision. Amsterdam, The Nether-lands:Springer, 2016. 630âˆ’645</a></li>
  <li><a href="">Xie S, Girshick R, Dollar P, Tu Z W, He K M. Aggregated residual transformations for deep neural networks. In: Proceedings of Computer Vision and Pattern Recognition. Honolulu, HI, USA: IEEE, 2017. 5987âˆ’5995</a></li>
  <li><a href="">LeCun Y, Bengio Y, Hinton G. Deep learning. Nature, 2015,521(7553): 436</a></li>
  <li><a href="https://arxiv.org/abs/1605.07146">Zagoruyko S, Komodakis N. Wide residual networks. arXiv:1605.07146, 2016.</a></li>
  <li><a href="https://arxiv.org/abs/1603.08029">Targ S, Almeida D, Lyman K. Resnet in resnet: generalizing residual architectures. arXiv: 1603.08029, 2016.</a></li>
  <li><a href="">Zhang K, Sun M, Han X, Yuan X F, Guo L R, Liu T. Residual networks of residual networks: multilevel residual networks. IEEE Transactions on Circuits and Systems for Video Technology, 2017: 1âˆ’1</a></li>
  <li><a href="https://arxiv.org/abs/1609.05672">Abdi M, Nahavandi S. Multi-residual networks. arXiv:1609.05672, 2016.</a></li>
  <li><a href="">Huang G, Liu Z, Weinberger K Q. Densely connected convolutional networks. In: Proceedings of Computer Vision and Pattern Recognition. Honolulu, HI, USA: IEEE, 2017.</a></li>
  <li><a href="">Zhang Ting, Li Yu-Jian, Hu Hai-He, Zhang Ya-Hong. A gender classification model based on cross-connected convolutional neural networks. Acta Automatica Sinica, 2016,42(6): 858-865( å¼ å©· , æç‰é‰´ , èƒ¡æµ·é¹¤ , å¼ äºšçº¢ . åŸºäºè·¨è¿å·ç§¯ç¥ç»ç½‘ç»œçš„æ€§åˆ«åˆ†ç±»æ¨¡å‹ . è‡ªåŠ¨åŒ–å­¦æŠ¥ , 2016, 42(6): 858-865)</a></li>
  <li><a href="">Li Yong, Lin Xiao-Zhu, Jiang Meng-Ying. Facial expression recognition with cross-connect LeNet-5 network. Acta Automatica Sinica, 2018, 44(1): 176-182( æå‹‡ , æ—å°ç«¹ , è’‹æ¢¦è¹ . åŸºäºè·¨è¿æ¥ LeNet-5 ç½‘ç»œçš„é¢éƒ¨è¡¨æƒ…è¯†åˆ« .è‡ªåŠ¨åŒ–å­¦æŠ¥ , 2018, 44(1): 176-182)</a></li>
  <li><a href="https://arxiv.org/abs/1704.04861">Howard A G, Zhu M, Chen B, Kalenichenko D. Mobilenets:efficient convolutional neural networks for mobile vision applications. arXiv: 1704.04861, 2017.</a></li>
  <li><a href="">Sandler M, Howard A, Zhu M, Zhmoginov A, Chen L C.MobileNetV2: inverted residuals and linear bottlenecks. In:Proceedings of Computer Vision and Pattern Recognition.USA: IEEE, 2018. 4510âˆ’4520</a></li>
  <li><a href="">Zhang X, Zhou X, Lin M, Sun J. ShuffleNet: an extremely efficient convolutional neural network for mobile devices. In:Proceedings of Computer Vision and Pattern Recognition.USA: IEEE, 2018.</a></li>
</ol>

:ET